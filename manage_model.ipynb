{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZzx7eWJX1L3jBwZO3gfvk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AtogmatomaProgramming/OdonScanAI/blob/develop/manage_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=== En caso de que no esté instalado scikit-learn ni tensorflow ===\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "LMtBGGxhCJNK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neY6DlY281Dl"
      },
      "outputs": [],
      "source": [
        "#=== Importar librerias ===\n",
        "\n",
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from google.colab import drive\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=== Funciones de trabajo ===\n",
        "\n",
        "# Función para obtener rutas de imágenes y etiquetas\n",
        "def obtener_imagenes_y_etiquetas(carpeta_base, archivo_json):\n",
        "    # Cargar o inicializar el JSON\n",
        "    if os.path.exists(archivo_json):\n",
        "        with open(archivo_json, 'r') as f:\n",
        "            especies_dict = json.load(f)\n",
        "    else:\n",
        "        especies_dict = {}\n",
        "\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for especie in sorted(os.listdir(carpeta_base)):\n",
        "        especie_dir = os.path.join(carpeta_base, especie)\n",
        "        if os.path.isdir(especie_dir):\n",
        "            if especie not in especies_dict:\n",
        "                especies_dict[especie] = len(especies_dict)\n",
        "            label = especies_dict[especie]\n",
        "\n",
        "            for root, _, files in os.walk(especie_dir):\n",
        "                for file in files:\n",
        "                    if file.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        image_paths.append(os.path.join(root, file))\n",
        "                        labels.append(label)\n",
        "\n",
        "    # Guardar el JSON actualizado\n",
        "    with open(archivo_json, 'w') as f:\n",
        "        json.dump(especies_dict, f)\n",
        "\n",
        "    return image_paths, labels\n",
        "\n",
        "# Función para preparar el dataset utilizando tf.data\n",
        "def preparar_datasets(image_paths, labels, batch_size=32, validation_split=0.2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "\n",
        "    # Cargar y preprocesar las imágenes\n",
        "    def cargar_y_preprocesar(ruta, etiqueta):\n",
        "        imagen = tf.io.read_file(ruta)\n",
        "        imagen = tf.image.decode_jpeg(imagen, channels=3)\n",
        "        imagen = tf.image.resize(imagen, [224, 224])\n",
        "        imagen = tf.image.convert_image_dtype(imagen, tf.float32)  # Normalizar\n",
        "        return imagen, etiqueta\n",
        "\n",
        "    dataset = dataset.map(cargar_y_preprocesar, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Dividir en entrenamiento y validación\n",
        "    dataset_size = len(image_paths)\n",
        "    train_size = int((1 - validation_split) * dataset_size)\n",
        "\n",
        "    train_ds = dataset.take(train_size)\n",
        "    val_ds = dataset.skip(train_size)\n",
        "\n",
        "    # Configurar el dataset para batching, mezclado y cache\n",
        "    train_ds = train_ds.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "# Definir el modelo\n",
        "def crear_modelo(num_clases):\n",
        "    modelo = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_clases, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    modelo.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "    return modelo\n",
        "\n",
        "\n",
        "\n",
        "# Función principal\n",
        "def main(carpeta_base, archivo_json, model_name):\n",
        "    # Crear rutas de imágenes y etiquetas desde las carpetas de clases\n",
        "    image_paths, labels = obtener_imagenes_y_etiquetas(carpeta_base, archivo_json)\n",
        "\n",
        "    # Preparar los datasets de entrenamiento y validación\n",
        "    train_ds, val_ds = preparar_datasets(image_paths, labels)\n",
        "\n",
        "    # Crear el modelo con el número de clases detectado\n",
        "    modelo = crear_modelo(num_clases=len(set(labels)))\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = modelo.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "\n",
        "    # Guardar el modelo entrenado\n",
        "    modelo.save(model_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "a7RlmJEV9Sgd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=== Ejecucion del script ===\n",
        "\n",
        "carpeta_base = \"/content/drive/My Drive/alpha_training/frames\"\n",
        "archivo_json= \"/content/drive/My Drive/alpha_training/etiquetas.json\"\n",
        "model_name = \"/content/drive/My Drive/alpha_training/modelo_gamma.h5\"\n",
        "\n",
        "main(carpeta_base, archivo_json, model_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1rDyMhpGoQbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}